{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joint-defensive",
   "metadata": {},
   "source": [
    "## Text to num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-conflict",
   "metadata": {},
   "source": [
    "i feel hungry   \n",
    "i eat lunch   \n",
    "now i feel happy   \n",
    "문장을 number로 바꿔보자,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cutting-solomon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-interference",
   "metadata": {},
   "source": [
    "모든 문장들이 들어있는 dictionary 생성!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eleven-column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-victim",
   "metadata": {},
   "source": [
    "dictionary에서 **key**를 단어 명으로 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "empirical-passage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word:index for index, word in index_to_word.items()} # 'word' : index 형태로 교체\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "theoretical-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-polymer",
   "metadata": {},
   "source": [
    "I eat lunch -> 1, 3, 6, 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "optional-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "#word_to_index에 word가 있으면 불러오고 아니면 unk. sentence를 split해서 얻은 word를 하나씩 넣어서 확인\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-lyric",
   "metadata": {},
   "source": [
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developed-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-audience",
   "metadata": {},
   "source": [
    "decode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "desperate-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "usual-avenue",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-portugal",
   "metadata": {},
   "source": [
    "#### embedding layer의 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-eleven",
   "metadata": {},
   "source": [
    "입력 문장 길이가 일정해야 함. ==> pad 필요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hidden-logan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8c0dfc25fa8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# list 형태의 sentences는 numpy array로 변환되어야 딥러닝 레이어의 입력이 될 수 있습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mraw_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_encoded_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m     if any(isinstance(x, (\n\u001b[1;32m    959\u001b[0m         np_arrays.ndarray, np.ndarray, float, int)) for x in input_list):\n\u001b[0;32m--> 960\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_convert_numpy_or_python_types\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3307\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3308\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3310\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1405\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# 아래 코드는 그대로 실행하시면 에러가 발생할 것입니다. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 위 그림과 같이 4차원의 워드 벡터를 가정합니다. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \n",
    "# list 형태의 sentences는 numpy array로 변환되어야 딥러닝 레이어의 입력이 될 수 있습니다.\n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index))\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "empty-timeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 4 5 0]\n",
      " [1 3 6 7 0]\n",
      " [1 8 3 4 9]]\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-sitting",
   "metadata": {},
   "source": [
    "embedding 완료!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "liable-london",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.03548503  0.0022894  -0.00175103  0.03469973]\n",
      "  [ 0.04880131 -0.01560261  0.0258549   0.00650574]\n",
      "  [-0.01696942  0.01897656 -0.0356712   0.00068833]\n",
      "  [-0.03822314 -0.01071631 -0.00058706 -0.02608924]\n",
      "  [ 0.02976942  0.04142389  0.02750511 -0.0443472 ]]\n",
      "\n",
      " [[ 0.03548503  0.0022894  -0.00175103  0.03469973]\n",
      "  [ 0.04880131 -0.01560261  0.0258549   0.00650574]\n",
      "  [-0.03355732 -0.02560073  0.01803165 -0.00450401]\n",
      "  [ 0.03330823 -0.01641413  0.03610767  0.00828133]\n",
      "  [ 0.02976942  0.04142389  0.02750511 -0.0443472 ]]\n",
      "\n",
      " [[ 0.03548503  0.0022894  -0.00175103  0.03469973]\n",
      "  [ 0.02792119 -0.00765242 -0.00989411 -0.04617416]\n",
      "  [ 0.04880131 -0.01560261  0.0258549   0.00650574]\n",
      "  [-0.01696942  0.01897656 -0.0356712   0.00068833]\n",
      "  [ 0.00297369 -0.03907963 -0.02925416 -0.03224798]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index))\n",
    "raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-marina",
   "metadata": {},
   "source": [
    "#### keras.lstm으로 구현한 word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bored-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-guess",
   "metadata": {},
   "source": [
    "### IMDb 데이터셋 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fancy-texture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000) #num_words : word dict에 저장할 단어 개수\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-yukon",
   "metadata": {},
   "source": [
    "텍스트가 아닌 encoding된 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comfortable-absence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-choir",
   "metadata": {},
   "source": [
    "encode된 값과 단어 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "double-alarm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-constitutional",
   "metadata": {},
   "source": [
    "word_to_index : 단어 출현 빈도 기준 내림차순 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "higher-expert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "direct-somewhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-trout",
   "metadata": {},
   "source": [
    "#### pad_sequences 활용해 문장의 길이 통일. 최대 길이 maxlen은 데이터셋의 분포를 통해서 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "painted-contract",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text] #데이터셋 마다 num_token 구하기!\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-medline",
   "metadata": {},
   "source": [
    "padding의 위치 post , pre 냐에 따라서 RNN 결과 달라짐. RNN의 경우 입력 데이터가 순차적으로 처리돼 가장 마지막 입력이 최종 state값에 가장 영향을 많이 받게 되므로 마지막에 무의미한 padding으로 채워진다면 비효율적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "processed-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-arabic",
   "metadata": {},
   "source": [
    "#### RNN 모델 설계 (임의 모델 입력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "challenging-activation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. \n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-integral",
   "metadata": {},
   "source": [
    "1 - D CNN 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단순 예시\n",
    "'''\n",
    "vocab_size = 10000  # 어휘 사전의 크기입니다\n",
    "word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "still-construction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 580)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 5000건 분리\n",
    "x_val = x_train[:5000]   \n",
    "y_val = y_train[:5000]\n",
    "\n",
    "# validation set을 제외한 나머지 20000건\n",
    "partial_x_train = x_train[5000:]  \n",
    "partial_y_train = y_train[5000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "hired-pontiac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[512,300,1,580] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential_3/conv1d/conv1d/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/sequential_3/embedding_4/embedding_lookup/Reshape/_44]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[512,300,1,580] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential_3/conv1d/conv1d/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_25648]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4ff6bef8b1b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                    callbacks = callback)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[512,300,1,580] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential_3/conv1d/conv1d/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/sequential_3/embedding_4/embedding_lookup/Reshape/_44]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[512,300,1,580] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential_3/conv1d/conv1d/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_25648]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-stylus",
   "metadata": {},
   "source": [
    "모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "traditional-quest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 22s - loss: 0.6923 - accuracy: 0.5095\n",
      "[0.6922616362571716, 0.5094799995422363]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-price",
   "metadata": {},
   "source": [
    "#### history 저장된 데이터 이용해서 학습 곡선 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "inner-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "painful-thousand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "filled-toronto",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-finish",
   "metadata": {},
   "source": [
    "### Word2Vec 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "civil-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "norwegian-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-exploration",
   "metadata": {},
   "source": [
    "gensim 이용해 임베딩 파라미터를 word_vector로 활용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "tribal-split",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06479677, -0.03158346,  0.09510023, -0.01630652, -0.09016483,\n",
       "        0.00487235,  0.01868608,  0.09225659,  0.02709491, -0.05439583,\n",
       "        0.08733477,  0.00635809, -0.09056619,  0.10485879,  0.06245336,\n",
       "        0.06916761], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-processor",
   "metadata": {},
   "source": [
    "#### 유사한 단어 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "respective-argument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('continuing', 0.9510118961334229),\n",
       " ('peters', 0.9044999480247498),\n",
       " ('mouths', 0.8920835852622986),\n",
       " ('click', 0.8902184367179871),\n",
       " ('makes', 0.8887825012207031),\n",
       " ('expression', 0.8887801766395569),\n",
       " ('chiba', 0.8796756863594055),\n",
       " ('notes', 0.878076434135437),\n",
       " ('rolls', 0.8772045969963074),\n",
       " ('shortcomings', 0.8770627379417419)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-enclosure",
   "metadata": {},
   "source": [
    "#### word_to_vec 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "positive-doctrine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000) #limit : 가장 많이 사용되는 상위 100만개만. limit = None : 300만개 전부 다. 요구 성능 높음.\n",
    "vector = word2vec['computer']\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "infrared-tulsa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907791495323181),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.661863386631012),\n",
       " ('passion', 0.6100708842277527),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547304749488831),\n",
       " ('absolutely_adore', 0.5536840558052063),\n",
       " ('adores', 0.5440906882286072)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-designation",
   "metadata": {},
   "source": [
    "임베딩이 굉장히 잘 돼있다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-collins",
   "metadata": {},
   "source": [
    "word2vec의 embedding 층 그대로 사용하기 위해서 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "unlimited-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-benjamin",
   "metadata": {},
   "source": [
    "모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "stretch-grass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 580, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 574, 16)           33616     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 114, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 108, 16)           1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,035,569\n",
      "Trainable params: 3,035,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원 수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "spatial-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/79 [==============================] - 20s 226ms/step - loss: 0.6934 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 15s 184ms/step - loss: 0.6931 - accuracy: 0.4937 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.6931 - accuracy: 0.5008 - val_loss: 0.6920 - val_accuracy: 0.4998\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.6931 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 7/20\n",
      "79/79 [==============================] - 15s 188ms/step - loss: 0.6931 - accuracy: 0.5054 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 8/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.6931 - accuracy: 0.5084 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 9/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 10/20\n",
      "79/79 [==============================] - 15s 187ms/step - loss: 0.6931 - accuracy: 0.5107 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 11/20\n",
      "79/79 [==============================] - 15s 187ms/step - loss: 0.6931 - accuracy: 0.5025 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 12/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.6931 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 13/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.6931 - accuracy: 0.5029 - val_loss: 0.6933 - val_accuracy: 0.4908\n",
      "Epoch 14/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6933 - val_accuracy: 0.4908\n",
      "Epoch 15/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.6931 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4908\n",
      "Epoch 16/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.6932 - accuracy: 0.4971 - val_loss: 0.6933 - val_accuracy: 0.4908\n",
      "Epoch 17/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.6931 - accuracy: 0.5053 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 18/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.4908\n",
      "Epoch 19/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.6931 - accuracy: 0.5018 - val_loss: 0.6933 - val_accuracy: 0.4908\n",
      "Epoch 20/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.6931 - accuracy: 0.5050 - val_loss: 0.6932 - val_accuracy: 0.4908\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "exterior-trust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 8s - loss: 0.6932 - accuracy: 0.5000\n",
      "[0.6931577920913696, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
