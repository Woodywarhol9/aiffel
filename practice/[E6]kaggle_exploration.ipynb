{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "import os\r\n",
    "from os.path import join\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import missingno as msno\r\n",
    "\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor\r\n",
    "from sklearn.model_selection import KFold, cross_val_score\r\n",
    "import xgboost as xgb\r\n",
    "import lightgbm as lgb\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "%config InlineBackend.figure_format = 'retina'\r\n",
    "\r\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "38bfbd5ada16035c96bb21265c51de80361f17b4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data' #cloud 환경으로 경로 변경\r\n",
    "\r\n",
    "train_data_path = join(data_dir,'train.csv')\r\n",
    "sub_data_path = join(data_dir, 'test.csv')"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "7d34243045b8e681bf168dc908a9388a2ceb5fa3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 데이터 살펴보기\n",
    "pandas의 read_csv 함수를 사용해 데이터를 읽어오고, 각 변수들이 나타내는 의미를 살펴보겠습니다.\n",
    "1. ID : 집을 구분하는 번호\n",
    "2. date : 집을 구매한 날짜\n",
    "3. price : 타겟 변수인 집의 가격\n",
    "4. bedrooms : 침실의 수\n",
    "5. bathrooms : 침실당 화장실 개수\n",
    "6. sqft_living : 주거 공간의 평방 피트\n",
    "7. sqft_lot : 부지의 평방 피트\n",
    "8. floors : 집의 층 수\n",
    "9. waterfront : 집의 전방에 강이 흐르는지 유무 (a.k.a. 리버뷰)\n",
    "10. view : 집이 얼마나 좋아 보이는지의 정도\n",
    "11. condition : 집의 전반적인 상태\n",
    "12. grade : King County grading 시스템 기준으로 매긴 집의 등급\n",
    "13. sqft_above : 지하실을 제외한 평방 피트\n",
    "14. sqft_basement : 지하실의 평방 피트\n",
    "15. yr_built : 집을 지은 년도\n",
    "16. yr_renovated : 집을 재건축한 년도\n",
    "17. zipcode : 우편번호\n",
    "18. lat : 위도\n",
    "19. long : 경도\n",
    "20. sqft_living15 : 2015년 기준 주거 공간의 평방 피트(집을 재건축했다면, 변화가 있을 수 있음)\n",
    "21. sqft_lot15 : 2015년 기준 부지의 평방 피트(집을 재건축했다면, 변화가 있을 수 있음)"
   ],
   "metadata": {
    "_uuid": "7f0d3320a32138c92ed7b0629a57bd5a719603b0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = pd.read_csv(train_data_path)\r\n",
    "sub = pd.read_csv(sub_data_path)\r\n",
    "print('train data dim : {}'.format(data.shape))\r\n",
    "print('sub data dim : {}'.format(sub.shape)) #price column x"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/aiffel/aiffel/kaggle_kakr_housing/data/train.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-04f7721da341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train data dim : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sub data dim : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#price column x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m         )\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m             )\n\u001b[1;32m    646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/aiffel/aiffel/kaggle_kakr_housing/data/train.csv'"
     ]
    }
   ],
   "metadata": {
    "_uuid": "df5891d33b4d5f08c0011b712f8796417564ec17"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "label column 제거"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = data['price']\r\n",
    "\r\n",
    "del data['price']"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "251c25b4f9c89db8b6643448369198e94437cfd7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "데이터 탐색 위해서 합쳐서 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_len = len(data) #나중에 학습 데이터만 불러올 수 있도록\n",
    "data = pd.concat((data, sub), axis=0)"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "dc5d87141207fedeb521461bd0d0cc23c0c594f2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.head()"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "96537247c40a4932e581a5c681d73348f6b3936b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 간단한 전처리 \n",
    "각 변수들에 대해 결측 유무를 확인하고, 분포를 확인해보면서 간단하게 전처리를 하겠습니다.\n",
    "### 결측치 확인\n",
    "먼저 데이터에 결측치가 있는지를 확인하겠습니다.<br>\n",
    "missingno 라이브러리의 matrix 함수를 사용하면, 데이터의 결측 상태를 시각화를 통해 살펴볼 수 있습니다."
   ],
   "metadata": {
    "_uuid": "39cc64c64be8d6f2ebd5c7c2523973bbcfb29c94"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "msno.matrix(data)"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "4e8ce87398995d2f86fc5cb7b15294f46ece7801"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "결측치 : 흰색"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "모든 변수에 결측치가 없는 것으로 보이지만, 혹시 모르니 확실하게 살펴보겠습니다.<br>"
   ],
   "metadata": {
    "_uuid": "a89a32c9e3491c45bf3188d22a40a211e3850d66"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[pd.isnull(data[c]),c] ==> data[c]에서의 결측값 확인(전체 series 출력). c column만 출력 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "isnull.sum()랑 다른 점??..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for c in data.columns:\r\n",
    "    print('{} : {}'.format(c, len(data.loc[pd.isnull(data[c]), c].values)))\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "6f406f14203d742ff26da3454935aa05c8364922",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.loc[pd.isnull(data['view']),'long']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### id, date 변수 정리\n",
    "id 변수는 모델이 집값을 예측하는데 도움을 주지 않으므로 제거합니다.<br>\n",
    "date 변수는 연월일시간으로 값을 가지고 있는데, 연월만 고려하는 범주형 변수로 만들겠습니다."
   ],
   "metadata": {
    "_uuid": "94ad23d7a91a68c1be7fcd4953013f5353541545"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub_id = data['id'][train_len:]\r\n",
    "del data['id']\r\n",
    "data['date'] = data['date'].apply(lambda x : str(x[:6])).astype(str)"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "6c48e1c16ec0725128286a0d524faff9bd725ed4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 각 변수들의 분포 확인\n",
    "한쪽으로 치우친 분포는 모델이 결과를 예측하기에 좋지 않은 영향을 미치므로 다듬어줄 필요가 있습니다."
   ],
   "metadata": {
    "_uuid": "200283a8d82a4f8c1951013961b5af3f91cc8be1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(9, 2, figsize=(12, 50))\r\n",
    "\r\n",
    "# id 변수는 제외하고 분포를 확인합니다.\r\n",
    "count = 1\r\n",
    "columns = data.columns\r\n",
    "for row in range(9):\r\n",
    "    for col in range(2):\r\n",
    "        sns.kdeplot(data[columns[count]], ax=ax[row][col])\r\n",
    "        ax[row][col].set_title(columns[count], fontsize=15)\r\n",
    "        count+=1\r\n",
    "        if count == 19 :\r\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "c2aa785314a8bb8d9f84bbc17cb55e38c564a8f3",
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "price, bedrooms, sqft_living, sqft_lot, sqft_above, sqft_basement 변수가 한쪽으로 치우친 경향을 보였습니다.<br>\n",
    "log-scaling을 통해 데이터 분포를 정규분포에 가깝게 만들어 보겠습니다."
   ],
   "metadata": {
    "_uuid": "b377cd53ae25ab16f09d303c5cf0019bc004f465"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "log 화 : 넓은 것을 좁게 , 좁은 것을 넓게"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "skew_columns = ['bedrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']\r\n",
    "\r\n",
    "for c in skew_columns:\r\n",
    "    data[c] = np.log1p(data[c].values)"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "24bb873fdfcdf1511b75aff69456b49d36f8f637"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(10, 15))\r\n",
    "\r\n",
    "count = 0\r\n",
    "for row in range(3):\r\n",
    "    for col in range(2):\r\n",
    "        if count == 5:\r\n",
    "            break\r\n",
    "        sns.kdeplot(data[skew_columns[count]], ax=ax[row][col])\r\n",
    "        ax[row][col].set_title(skew_columns[count], fontsize=15)\r\n",
    "        count+=1\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "876da0a04810168164a97239fc809ea38b277cdb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "어느정도 치우침이 줄어든 분포를 확인할 수 있습니다."
   ],
   "metadata": {
    "_uuid": "5923d97d51ae10050473c02eb8d56afa6d3e7567"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub = data.iloc[train_len:, :]\r\n",
    "x = data.iloc[:train_len, :]"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "b4d31773ba934e8b48564fb8207df9e574b3a195"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 모델링\n",
    "### Average Blending\n",
    "여러가지 모델의 결과를 산술 평균을 통해 Blending 모델을 만들겠습니다."
   ],
   "metadata": {
    "_uuid": "a4afe676edcf34f694c95c3f0e6287d13edaee26"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gboost = GradientBoostingRegressor(random_state=2019)\n",
    "xgboost = xgb.XGBRegressor(random_state=2019)\n",
    "lightgbm = lgb.LGBMRegressor(random_state=2019)\n",
    "\n",
    "models = [{'model':gboost, 'name':'GradientBoosting'}, {'model':xgboost, 'name':'XGBoost'},\n",
    "          {'model':lightgbm, 'name':'LightGBM'}]"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "296645421a135dc34c45f822ce73d31eeb107bdc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross Validation\n",
    "교차 검증을 통해 모델의 성능을 간단히 평가하겠습니다."
   ],
   "metadata": {
    "_uuid": "578896883c6b8c6a2c5785a93121b62506b22a21"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_cv_score(models):\n",
    "    kfold = KFold(n_splits=5, random_state=2019).get_n_splits(x.values)\n",
    "    for m in models:\n",
    "        print(\"Model {} CV score : {:.4f}\".format(m['name'], np.mean(cross_val_score(m['model'], x.values, y)), \n",
    "                                             kf=kfold)) #kf안쓰이는건가??"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "4b48689d7989107ea3524126c330e2225a58e4d3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_cv_score(models)"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "53ea586dd0b4fe5481db7bfb9cbcb998ef88b648"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make Submission"
   ],
   "metadata": {
    "_uuid": "a26c1d6aa9b779077f84ec60b7f29385fc2e058b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "회귀 모델의 경우에는 cross_val_score 함수가 R<sup>2</sup>를 반환합니다.<br>\n",
    "R<sup>2</sup> 값이 1에 가까울수록 모델이 데이터를 잘 표현함을 나타냅니다. 3개 트리 모델이 상당히 훈련 데이터에 대해 괜찮은 성능을 보여주고 있습니다.<br> 훈련 데이터셋으로 3개 모델을 학습시키고, Average Blending을 통해 제출 결과를 만들겠습니다."
   ],
   "metadata": {
    "_uuid": "1004d9b01d1c3068d9e8a81f263315435478e591"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def AveragingBlending(models, x, y, sub_x):\n",
    "    for m in models : \n",
    "        m['model'].fit(x.values, y)\n",
    "    \n",
    "    predictions = np.column_stack([\n",
    "        m['model'].predict(sub_x.values) for m in models #for문 뒤에 써도 되는구나.\n",
    "    ])\n",
    "    return np.mean(predictions, axis=1)"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "20af2394b94bdc14fe1b23688874fadbf1c2846a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = AveragingBlending(models, x, y, sub) #x , y : 훈련 데이터"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "f7bdb1c740291e6d1721cf02cd3f59bb79153c87"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "submission csv파일 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\r\n",
    "\r\n",
    "submission_path = join(data_dir, 'sample_submission.csv')\r\n",
    "submission = pd.read_csv(submission_path)\r\n",
    "submission.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "csv 파일 저장"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = pd.DataFrame(data={'id':sub_id,'price':y_pred})"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "9c9d0901bcf5e4754b40a6922292c04951b81bae"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_submission_path = join(data_dir, 'submission.csv')\r\n",
    "result.to_csv(my_submission_path, index=False)\r\n",
    "\r\n",
    "print(my_submission_path)"
   ],
   "outputs": [],
   "metadata": {
    "_uuid": "2047d6ee5f5da05e5463d51d1fbbe82a94732a48"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "____________________________________________"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 직접 튜닝해보기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data 다시 불러오기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\r\n",
    "\r\n",
    "train_data_path = join(data_dir, 'train.csv')\r\n",
    "test_data_path = join(data_dir, 'test.csv') \r\n",
    "\r\n",
    "train = pd.read_csv(train_data_path)\r\n",
    "test = pd.read_csv(test_data_path)\r\n",
    "\r\n",
    "print('얍💢')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.head()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "data를 int형으로 바꿔 예측을 위한 특성으로 사용하자."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train['date'] = train['date'].apply(lambda i: i[:6]).astype(int)\r\n",
    "train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "target data인 price를 따로 y로 빼서 저장."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = train['price']\r\n",
    "del train['price']\r\n",
    "\r\n",
    "print(train.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "예측에 도움되지 않는 id column 삭제"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del train['id']\r\n",
    "\r\n",
    "print(train.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "test data에 대해서도 똑같이 진행"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test['date'] = test['date'].apply(lambda i: i[:6]).astype(int)\r\n",
    "\r\n",
    "del test['id']\r\n",
    "\r\n",
    "print(test.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "y 분포 시각화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.kdeplot(y)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "왼쪽으로 크게 치우쳐진 형태. np.log1p() 함수 통해 정규화후 나중에 predict 결과를 np.exp1m()을 통해 되돌린다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = np.log1p(y)\r\n",
    "y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.kdeplot(y)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "데이터 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 튜닝을 위해선 반복되는 작업을 함수로 묶는 것이 필요하다!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "print('얍💢')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "log화 된 예측 결과이므로 RMSE 계산 전에 np.expm1을 통해서 다시 되돌린다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rmse(y_test, y_pred):\r\n",
    "    return np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))\r\n",
    "\r\n",
    "print('얍💢')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델 load"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from xgboost import XGBRegressor\r\n",
    "from lightgbm import LGBMRegressor\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\r\n",
    "\r\n",
    "print('얍💢')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# random_state는 모델초기화나 데이터셋 구성에 사용되는 랜덤 시드값입니다. \r\n",
    "#random_state=None    # 이게 초기값입니다. 아무것도 지정하지 않고 None을 넘겨주면 모델 내부에서 임의로 선택합니다.  \r\n",
    "random_state=2020        # 하지만 우리는 이렇게 고정값을 세팅해 두겠습니다. \r\n",
    "\r\n",
    "gboost = GradientBoostingRegressor(random_state=random_state)\r\n",
    "xgboost = XGBRegressor(random_state=random_state)\r\n",
    "lightgbm = LGBMRegressor(random_state=random_state)\r\n",
    "rdforest = RandomForestRegressor(random_state=random_state)\r\n",
    "\r\n",
    "models = [gboost, xgboost, lightgbm, rdforest]\r\n",
    "\r\n",
    "print('얍💢')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "class 이름 얻기 : 이를 통해서 모델 명 접근 가능!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "gboost.__class__.__name__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = {}\r\n",
    "\r\n",
    "for model in models:\r\n",
    "    # 모델 이름 획득\r\n",
    "    model_name = model.__class__.__name__\r\n",
    "\r\n",
    "    # train, test 데이터셋 분리 - 여기에도 random_state를 고정합니다. \r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, y, random_state=random_state, test_size=0.2)\r\n",
    "\r\n",
    "    # 모델 학습\r\n",
    "    model.fit(X_train, y_train)\r\n",
    "    \r\n",
    "    # 예측\r\n",
    "    y_pred = model.predict(X_test)\r\n",
    "\r\n",
    "    # 예측 결과의 rmse값 저장\r\n",
    "    df[model_name] = rmse(y_test, y_pred)\r\n",
    "    \r\n",
    "    # data frame에 저장\r\n",
    "    score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)\r\n",
    "    \r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "함수화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_scores(models, train, y):\r\n",
    "    \r\n",
    "    df = {}\r\n",
    "    \r\n",
    "    for model in models:\r\n",
    "    # 모델 이름 획득\r\n",
    "    \tmodel_name = model.__class__.__name__\r\n",
    "\r\n",
    "    \t# train, test 데이터셋 분리 - 여기에도 random_state를 고정합니다. \r\n",
    "    \tX_train, X_test, y_train, y_test = train_test_split(train, y, random_state=random_state, test_size=0.2)\r\n",
    "\r\n",
    "    \t# 모델 학습\r\n",
    "    \tmodel.fit(X_train, y_train)\r\n",
    "    \r\n",
    "    \t# 예측\r\n",
    "    \ty_pred = model.predict(X_test)\r\n",
    "\r\n",
    "    \t# 예측 결과의 rmse값 저장\r\n",
    "    \tdf[model_name] = rmse(y_test, y_pred)\r\n",
    "    \r\n",
    "    \t# data frame에 저장\r\n",
    "    \tscore_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)\r\n",
    "    return score_df\r\n",
    "\r\n",
    "get_scores(models, train, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Grid search 이용해 하이퍼 파라미터 튜닝"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print('얍💢')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GridSearchCV에 입력되는 인자들은 다음과 같습니다.\n",
    "\n",
    "- param_grid : 탐색할 파라미터의 종류 (딕셔너리로 입력)\n",
    "- scoring : 모델의 성능을 평가할 지표\n",
    "- cv : cross validation을 수행하기 위해 train 데이터셋을 나누는 조각의 개수\n",
    "- verbose : 그리드 탐색을 진행하면서 진행 과정을 출력해서 보여줄 메세지의 양 (숫자가 클수록 더 많은 메세지를 출력합니다.)\n",
    "- n_jobs : 그리드 탐색을 진행하면서 사용할 CPU의 개수"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [1, 10],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = LGBMRegressor(random_state=random_state)\r\n",
    "\r\n",
    "print('얍💢')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "param 경우의 수 : 4 , cross validation 수 5 따라서 총 실행 수는 20"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_model = GridSearchCV(model, param_grid=param_grid, \\\r\n",
    "                        scoring='neg_mean_squared_error', \\\r\n",
    "                        cv=5, verbose=1, n_jobs=5)\r\n",
    "\r\n",
    "grid_model.fit(train, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "결과 출력"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_model.cv_results_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "파라미터 조합만 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = grid_model.cv_results_['params']\r\n",
    "params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "테스트 점수 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score = grid_model.cv_results_['mean_test_score']\r\n",
    "score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "DataFrame으로 바꾸기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = pd.DataFrame(params)\r\n",
    "results['score'] = score\r\n",
    "\r\n",
    "results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "결과가 왜 음수 ? scoring 인자에 neg_mean_squared_error를 입력했기 때문 --> rmse로 바꾸자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results['RMSE'] = np.sqrt(-1 * results['score'])\r\n",
    "results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "결과의 scale은 price의 scale을 조정했기 때문. RMSE가 아닌 RMSLE(log)로 변경"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = results.rename(columns={'RMSE': 'RMSLE'})\r\n",
    "results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "RMSLE 낮은 순서대로 정렬"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = results.sort_values('RMSLE')\r\n",
    "results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 함수화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5):\r\n",
    "    # GridSearchCV 모델로 초기화\r\n",
    "    grid_model = GridSearchCV(model, param_grid=param_grid, scoring='neg_mean_squared_error', \\\r\n",
    "                              cv=5, verbose=verbose, n_jobs=n_jobs)\r\n",
    "    \r\n",
    "    # 모델 fitting\r\n",
    "    grid_model.fit(train, y)\r\n",
    "\r\n",
    "    # 결과값 저장\r\n",
    "    params = grid_model.cv_results_['params']\r\n",
    "    score = grid_model.cv_results_['mean_test_score']\r\n",
    "    \r\n",
    "    # 데이터 프레임 생성\r\n",
    "    results = pd.DataFrame(params)\r\n",
    "    results['score'] = score\r\n",
    "    \r\n",
    "    # RMSLE 값 계산 후 정렬\r\n",
    "    results['RMSLE'] = np.sqrt(-1 * results['score'])\r\n",
    "    results = results.sort_values('RMSLE')\r\n",
    "\r\n",
    "    return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 제출 과정 함수화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_grid = {\r\n",
    "    'n_estimators': [50, 100],\r\n",
    "    'max_depth': [1, 10],\r\n",
    "}\r\n",
    "\r\n",
    "model = LGBMRegressor(random_state=random_state)\r\n",
    "my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "결과로 얻은 가장 좋은 조합을 통해 학습 후 csv 파일로 만들자!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = LGBMRegressor(max_depth=10, n_estimators=100, random_state=random_state)\r\n",
    "model.fit(train, y)\r\n",
    "prediction = model.predict(test)\r\n",
    "prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "scale 변환"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction = np.expm1(prediction)\r\n",
    "prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "csv 파일로 저장"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\r\n",
    "\r\n",
    "submission_path = join(data_dir, 'sample_submission.csv')\r\n",
    "submission = pd.read_csv(submission_path)\r\n",
    "submission.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "price 덮어 씌우기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission['price'] = prediction\r\n",
    "submission.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission_csv_path = '{}/submission_{}_RMSLE_{}.csv'.format(data_dir, 'lgbm', '0.164399')\r\n",
    "submission.to_csv(submission_csv_path, index=False)\r\n",
    "print(submission_csv_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 함수화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def save_submission(model, train, y, test, model_name, rmsle=None):\r\n",
    "    model.fit(train, y)\r\n",
    "    prediction = model.predict(test)\r\n",
    "    prediction = np.expm1(prediction)\r\n",
    "    data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\r\n",
    "    submission_path = join(data_dir, 'sample_submission.csv')\r\n",
    "    submission = pd.read_csv(submission_path)\r\n",
    "    submission['price'] = prediction\r\n",
    "    submission_csv_path = '{}/submission_{}_RMSLE_{}.csv'.format(data_dir, model_name, rmsle)\r\n",
    "    submission.to_csv(submission_csv_path, index=False)\r\n",
    "    print('{} saved!'.format(submission_csv_path))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "save_submission(model, train, y, test, 'lgbm', rmsle='0.0168')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}