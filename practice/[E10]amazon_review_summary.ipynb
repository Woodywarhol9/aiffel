{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "urban-equipment",
   "metadata": {},
   "source": [
    "# Amazon 리뷰 요약하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-document",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리 및 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-turtle",
   "metadata": {},
   "source": [
    "stopwords 처리를 위해 NLTK 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "approximate-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-promotion",
   "metadata": {},
   "source": [
    "10만개의 샘플만 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pressed-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stable-aruba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-jonathan",
   "metadata": {},
   "source": [
    "#### 학습에 필요한 'Text' 와 'Summary'만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "surprised-factory",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60404</th>\n",
       "      <td>Although this product does have an abundance o...</td>\n",
       "      <td>Best natural energy boost I've ever taken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59588</th>\n",
       "      <td>Received this product a while ago but didn't m...</td>\n",
       "      <td>Yummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59259</th>\n",
       "      <td>I love this tea!  It's a simple, plain black t...</td>\n",
       "      <td>Plain simple black tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89453</th>\n",
       "      <td>Izze makes some very good juices, and advertis...</td>\n",
       "      <td>A tad too sweet for my taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38457</th>\n",
       "      <td>I've never tried any other himalayan salt but ...</td>\n",
       "      <td>Good salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86065</th>\n",
       "      <td>The dark roast coffee is not at all 'full bodi...</td>\n",
       "      <td>Very bland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53551</th>\n",
       "      <td>It's a bit sticky, but other than that it is o...</td>\n",
       "      <td>not too bad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16891</th>\n",
       "      <td>The Kiwi Berry drink may not have sugar addded...</td>\n",
       "      <td>Sweet sweet Kiwi Berry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17731</th>\n",
       "      <td>This stuff is amazing-it not only tastes exact...</td>\n",
       "      <td>Tastes EXACTLY LIKE SUGAR,No aftertaste!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62900</th>\n",
       "      <td>Marketers know that if they slap \"pure\" and \"n...</td>\n",
       "      <td>A good-tasting gimmick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84297</th>\n",
       "      <td>Great Taste, not greasey whatsoever like Slim ...</td>\n",
       "      <td>Very Tastey, Addiction Worthy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38094</th>\n",
       "      <td>great buy on these rice noodles.  it ends up b...</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35217</th>\n",
       "      <td>I was moved to try this coffee because of othe...</td>\n",
       "      <td>It's Ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10628</th>\n",
       "      <td>These are nutty delicious, and well packaged. ...</td>\n",
       "      <td>Everyday food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23310</th>\n",
       "      <td>I've been using these chews for years now and ...</td>\n",
       "      <td>Excellent product that works!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "60404  Although this product does have an abundance o...   \n",
       "59588  Received this product a while ago but didn't m...   \n",
       "59259  I love this tea!  It's a simple, plain black t...   \n",
       "89453  Izze makes some very good juices, and advertis...   \n",
       "38457  I've never tried any other himalayan salt but ...   \n",
       "86065  The dark roast coffee is not at all 'full bodi...   \n",
       "53551  It's a bit sticky, but other than that it is o...   \n",
       "16891  The Kiwi Berry drink may not have sugar addded...   \n",
       "17731  This stuff is amazing-it not only tastes exact...   \n",
       "62900  Marketers know that if they slap \"pure\" and \"n...   \n",
       "84297  Great Taste, not greasey whatsoever like Slim ...   \n",
       "38094  great buy on these rice noodles.  it ends up b...   \n",
       "35217  I was moved to try this coffee because of othe...   \n",
       "10628  These are nutty delicious, and well packaged. ...   \n",
       "23310  I've been using these chews for years now and ...   \n",
       "\n",
       "                                         Summary  \n",
       "60404  Best natural energy boost I've ever taken  \n",
       "59588                                      Yummy  \n",
       "59259                     Plain simple black tea  \n",
       "89453               A tad too sweet for my taste  \n",
       "38457                                  Good salt  \n",
       "86065                                 Very bland  \n",
       "53551                               not too bad.  \n",
       "16891                     Sweet sweet Kiwi Berry  \n",
       "17731   Tastes EXACTLY LIKE SUGAR,No aftertaste!  \n",
       "62900                     A good-tasting gimmick  \n",
       "84297             Very Tastey, Addiction Worthy.  \n",
       "38094                              great product  \n",
       "35217                                    It's Ok  \n",
       "10628                              Everyday food  \n",
       "23310              Excellent product that works!  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Text','Summary']]\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-projection",
   "metadata": {},
   "source": [
    "중복 , NULL 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absent-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-gauge",
   "metadata": {},
   "source": [
    "Summary는 동일할 수 있어도 Text는 동일할 수 없다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "numerical-practice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-visitor",
   "metadata": {},
   "source": [
    "NULL 확인 ( drop_duplicates 실행 시 한개의 NULL 빼고 제거되므로 NULL이 있다면 1개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "representative-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-library",
   "metadata": {},
   "source": [
    "단순 결측치라고 판단돼 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "superior-marine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33958</th>\n",
       "      <td>I only used two maybe three tea bags and got p...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Summary\n",
       "33958  I only used two maybe three tea bags and got p...     NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Summary'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-counter",
   "metadata": {},
   "source": [
    "NULL 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mathematical-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-billy",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-dictionary",
   "metadata": {},
   "source": [
    "text normalization 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "painted-rachel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-mobile",
   "metadata": {},
   "source": [
    "stopword 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "diagnostic-workplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "modified-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "graphic-chamber",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4391c9f99dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtemp_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Great way to start (or finish) the day!!!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 불용어를 제거하지 않습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-investigation",
   "metadata": {},
   "source": [
    "복잡한 작업엔 multiprocessing 사용하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "editorial-clinic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclean_text = []\\n# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \\nfor s in data['Text']:\\n    clean_text.append(preprocess_sentence(s))\\n\\n# 전처리 후 출력\\nprint(clean_text[:5])\\n\\n\\nclean_summary = []\\n# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \\nfor s in data['Summary']:\\n    clean_summary.append(preprocess_sentence(s, False))\\n\\nprint(clean_summary[:5])\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "clean_text = []\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(clean_text[:5])\n",
    "\n",
    "\n",
    "clean_summary = []\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(clean_summary[:5])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "saving-antique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226.72794604301453  seconds\n",
      "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better'\n",
      " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo'\n",
      " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch'\n",
      " ...\n",
      " 'favorite brand korean ramen spicy used eating spicy food make sure use spice pack add egg soup makes great snack'\n",
      " 'like noodles although say spicy somewhat understatement one else family tolerates spicy well seeing looking forward extra little something palate disappointed completely honest usually drain liquid almost much'\n",
      " 'love noodle twice week amazing thing feel well cold hot bowl noodle cure upset stomach headache running nose may work definitely try']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.488565921783447  seconds\n",
      "['good quality dog food' 'not as advertised' 'delight says it all' ...\n",
      " 'great ramen' 'spicy'\n",
      " 'this spicy noodle cures my cold upset stomach and headache every time']\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp   # 멀티 프로세싱으로 전처리 속도를 획기적으로 줄여봅시다\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial  # map을 할 때 함수에 여러 인자를 넣어줄 수 있도록 합니다\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# num_cores 만큼 쪼개진 데이터를 전처리하여 반환합니다\n",
    "def appendTexts(sentences, remove_stopwords):\n",
    "  texts = []\n",
    "  for s in sentences:\n",
    "    texts += preprocess_sentence(s, remove_stopwords),\n",
    "  return texts\n",
    "\n",
    "def preprocess_data(data, remove_stopwords=True):\n",
    "  start_time = time.time()\n",
    "  num_cores = mp.cpu_count()  # 컴퓨터의 코어 수를 구합니다\n",
    "\n",
    "  text_data_split = np.array_split(data, num_cores)  # 코어 수만큼 데이터를 배분하여 병렬적으로 처리할 수 있게 합니다\n",
    "  pool = Pool(num_cores)\n",
    "\n",
    "  processed_data = np.concatenate(pool.map(partial(appendTexts, remove_stopwords=remove_stopwords), text_data_split))  # 각자 작업한 데이터를 하나로 합쳐줍니다\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  print(time.time() - start_time, \" seconds\")\n",
    "  return processed_data\n",
    "\n",
    "clean_text = preprocess_data(data['Text'])  # 클라우드 기준으로 3~4분 정도 소요 됩니다\n",
    "print(clean_text)\n",
    "\n",
    "clean_summary = preprocess_data(data['Summary'], remove_stopwords=False) # 클라우드 기준 1분정도 소요됩니다.\n",
    "print(clean_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-municipality",
   "metadata": {},
   "source": [
    "빈 값(empty)를 Null로 대체 : isnull() 이용하기 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surprising-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acting-backing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "phantom-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-design",
   "metadata": {},
   "source": [
    "#### 샘플의 최대 길이 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "divided-electric",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-plasma",
   "metadata": {},
   "source": [
    "임의의 최대 문장 길이 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "specified-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "substantial-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cultural-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-configuration",
   "metadata": {},
   "source": [
    "len(x.split) : 단어 개수. text_max_len보다 작은 문장까지만 보관"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "norwegian-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65818\n"
     ]
    }
   ],
   "source": [
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-toyota",
   "metadata": {},
   "source": [
    "#### 시작과 끝에 sos, eos 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vulnerable-consequence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-louis",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nearby-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-notion",
   "metadata": {},
   "source": [
    "shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pleased-router",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6017  7769  8356 ... 62054 26774 39179]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(210210821)\n",
    "\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "liked-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-variable",
   "metadata": {},
   "source": [
    "전체 데이터의 20%를 Test data로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "julian-accordance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-extent",
   "metadata": {},
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "peaceful-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-printing",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-prior",
   "metadata": {},
   "source": [
    "#### vocabulary 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "thorough-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-scoop",
   "metadata": {},
   "source": [
    "등장 빈도수가 7보다 작은 단어들의 개수를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "comprehensive-summit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 32026\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23794\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8232\n",
      "단어 집합에서 희귀 단어의 비율: 74.29588459376757\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.403213340571084\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-devil",
   "metadata": {},
   "source": [
    "등장 빈도가 6회 이하인 단어는 전체 단어의 3.38% 정도 밖에 안된다. 따라서 이 경우 훈련 데이터에서 제외해주자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-links",
   "metadata": {},
   "source": [
    "fit_on_texts : 빈도 수 기준으로 index 부여. 따라서 src_vocab을 줄이면 빈도수 많은 문장들만 모이게 된다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "democratic-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 8000 #희귀 단어 제외시 단어 개수\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "foster-opera",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[345, 9, 1469, 3255, 576, 280, 802, 7, 199], [366, 148, 1522, 16, 258, 117, 349, 158, 215, 309, 4662, 869, 18, 180, 1364, 38, 93, 117, 645], [14, 1117, 3993, 2848, 548, 3993, 2655, 135, 982, 582, 509, 21, 982, 1229, 169, 3993, 2848]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-annex",
   "metadata": {},
   "source": [
    "decoder 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "institutional-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "contained-elimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10467\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8094\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2373\n",
      "단어 집합에서 희귀 단어의 비율: 77.32874749211808\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.912355888211505\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "designed-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 152], [1, 159], [1, 107, 12, 1363, 1967, 8, 683], [1, 711, 78, 11], [1, 206]]\n",
      "target\n",
      "decoder  [[152, 2], [159, 2], [107, 12, 1363, 1967, 8, 683, 2], [711, 78, 11, 2], [206, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "agreed-asian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1245\n",
      "삭제할 테스트 데이터의 개수 : 356\n",
      "훈련 데이터의 개수 : 51410\n",
      "훈련 레이블의 개수 : 51410\n",
      "테스트 데이터의 개수 : 12807\n",
      "테스트 레이블의 개수 : 12807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-height",
   "metadata": {},
   "source": [
    "padding 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "polyphonic-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-justice",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-invasion",
   "metadata": {},
   "source": [
    "## 2. 모델 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-vegetarian",
   "metadata": {},
   "source": [
    "#### Encoder 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "vertical-pattern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-lobby",
   "metadata": {},
   "source": [
    "#### Decoder 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "approximate-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-bouquet",
   "metadata": {},
   "source": [
    "마지막에 softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-screw",
   "metadata": {},
   "source": [
    "Summary 단어장인 tar_vocab 중 선택!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fantastic-africa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-syndrome",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "whole-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-reviewer",
   "metadata": {},
   "source": [
    "Layer가 훨씬 더 증가했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "italic-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,276,432\n",
      "Trainable params: 4,276,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-overview",
   "metadata": {},
   "source": [
    "## 3. 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-upper",
   "metadata": {},
   "source": [
    "validation_loss를 기준으로 early-stopping 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fleet-guatemala",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\\nes = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\\nhistory = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,           validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),           batch_size=256, callbacks=[es], epochs=50)\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-funeral",
   "metadata": {},
   "source": [
    "Epoch 1/50   \n",
    "201/201 [==============================] - 147s 678ms/step - loss: 3.1203 - val_loss: 2.4443   \n",
    "Epoch 2/50   \n",
    "201/201 [==============================] - 135s 670ms/step - loss: 2.4033 - val_loss: 2.3013   \n",
    "Epoch 3/50   \n",
    "201/201 [==============================] - 135s 674ms/step - loss: 2.2548 - val_loss: 2.1736   \n",
    "Epoch 4/50   \n",
    "201/201 [==============================] - 135s 674ms/step - loss: 2.1268 - val_loss: 2.0904   \n",
    "Epoch 5/50   \n",
    "201/201 [==============================] - 136s 677ms/step - loss: 2.0365 - val_loss: 2.0349   \n",
    "Epoch 6/50   \n",
    "201/201 [==============================] - 136s 675ms/step - loss: 1.9691 - val_loss: 1.9952   \n",
    "Epoch 7/50   \n",
    "201/201 [==============================] - 134s 665ms/step - loss: 1.9184 - val_loss: 1.9640   \n",
    "Epoch 8/50   \n",
    "201/201 [==============================] - 132s 657ms/step - loss: 1.8736 - val_loss: 1.9401   \n",
    "Epoch 9/50   \n",
    "201/201 [==============================] - 132s 655ms/step - loss: 1.8240 - val_loss: 1.9190   \n",
    "Epoch 10/50   \n",
    "201/201 [==============================] - 130s 648ms/step - loss: 1.7810 - val_loss: 1.9053   \n",
    "Epoch 11/50   \n",
    "201/201 [==============================] - 129s 641ms/step - loss: 1.7385 - val_loss: 1.8951   \n",
    "Epoch 12/50   \n",
    "201/201 [==============================] - 129s 641ms/step - loss: 1.7144 - val_loss: 1.8859   \n",
    "Epoch 13/50   \n",
    "201/201 [==============================] - 129s 640ms/step - loss: 1.6843 - val_loss: 1.8809   \n",
    "Epoch 14/50   \n",
    "201/201 [==============================] - 129s 640ms/step - loss: 1.6498 - val_loss: 1.8686   \n",
    "Epoch 15/50   \n",
    "201/201 [==============================] - 128s 639ms/step - loss: 1.6284 - val_loss: 1.8702   \n",
    "Epoch 16/50   \n",
    "201/201 [==============================] - 129s 640ms/step - loss: 1.5973 - val_loss: 1.8647   \n",
    "Epoch 17/50   \n",
    "201/201 [==============================] - 128s 639ms/step - loss: 1.5815 - val_loss: 1.8663   \n",
    "Epoch 18/50   \n",
    "201/201 [==============================] - 128s 638ms/step - loss: 1.5522 - val_loss: 1.8637   \n",
    "Epoch 19/50   \n",
    "201/201 [==============================] - 128s 636ms/step - loss: 1.5280 - val_loss: 1.8615   \n",
    "Epoch 20/50   \n",
    "201/201 [==============================] - 129s 640ms/step - loss: 1.5077 - val_loss: 1.8693   \n",
    "Epoch 21/50   \n",
    "201/201 [==============================] - 128s 636ms/step - loss: 1.4786 - val_loss: 1.8696   \n",
    "Epoch 00021: early stopping   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "waiting-native",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.plot(history.history['loss'], label='train')\\nplt.plot(history.history['val_loss'], label='test')\\nplt.legend()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-supplement",
   "metadata": {},
   "source": [
    "모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "simple-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/E10/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/E10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-mandate",
   "metadata": {},
   "source": [
    "#### 훈련이 오래 걸리기 때문에 저장한 모델을 불러와서 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "nervous-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "greatest-future",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "    \n",
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "    \n",
    "# 모델 정의\n",
    "loaded_model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ruled-manhattan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('saved_model/E10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "robust-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재사용할 layer 불러오기\n",
    "\n",
    "#encoder\n",
    "\n",
    "encoder_inputs = loaded_model.input[0] #input\n",
    "enc_emb = loaded_model.layers[1](encoder_inputs)\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = loaded_model.layers[2]\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = loaded_model.layers[4]\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = loaded_model.layers[6]\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "#encoder_outputs, state_h, state_c = loaded_model.layers[6].output #lstm_3(encoder-ends)\n",
    "\n",
    "\n",
    "#decoder\n",
    "decoder_inputs = loaded_model.input[1] #input\n",
    "dec_emb_layer = loaded_model.layers[5]\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = loaded_model.layers[7]\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "#Attention\n",
    "attn_layer = loaded_model.layers[8]\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "#Concatenate\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = loaded_model.layers[10]\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "moderate-civilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f2a36354610>,\n",
       " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f2a305fbc10>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f2a35e6ef90>,\n",
       " <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f2a35f342d0>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f2a35e6fb10>,\n",
       " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f2a340183d0>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f2a3294ee50>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f2bb074a510>,\n",
       " <tensorflow.python.keras.saving.saved_model.load.AttentionLayer at 0x7f2a361e7f50>,\n",
       " <tensorflow.python.keras.layers.merge.Concatenate at 0x7f2a33eeced0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f2a32a78d90>]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "pleased-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,276,432\n",
      "Trainable params: 4,276,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "alien-blocking",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'evalutate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-f16051059c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalutate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'evalutate'"
     ]
    }
   ],
   "source": [
    "loaded_model.evalutate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-throw",
   "metadata": {},
   "source": [
    "### 추론 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-asthma",
   "metadata": {},
   "source": [
    "Inference model 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-memorial",
   "metadata": {},
   "source": [
    "정수 인덱스로 얻어진 텍스트 데이터를 원본으로 돌리기 위해서 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "physical-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-palmer",
   "metadata": {},
   "source": [
    "정답 문장이 없어 만들어 낼 문장의 길이만큼 디코더가 반복 구조로 사용돼야 하므로 인코더 - 디코더를 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "connected-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-technology",
   "metadata": {},
   "source": [
    "Attention 출력층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "premium-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-clinic",
   "metadata": {},
   "source": [
    "#### inference sequence 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "included-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-battlefield",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-maria",
   "metadata": {},
   "source": [
    "## 4. 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-receipt",
   "metadata": {},
   "source": [
    "정수 sequence 입력을 텍스트 시퀀스로 추출해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "professional-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "permanent-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : two small doggies love food love benefits nutrition get highly recommended diet dog \n",
      "실제 요약 : doggie lovers food \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy cheesy extract regular\n",
      "\n",
      "\n",
      "원문 : enjoy tea soda time product clean crisp taste love organic factor tea \n",
      "실제 요약 : of china white tea \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract brew extract\n",
      "\n",
      "\n",
      "원문 : flowers may look little strange taste great mixed champagne afterward \n",
      "실제 요약 : great gift \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : go brand flavor since cannot find ramen subscribe save one top drawer choices ramen \n",
      "실제 요약 : great alternative \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : second order hidden springs maple syrup continue great price quick mention great syrup stay regular customers \n",
      "실제 요약 : nothing like maple syrup \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy cheesy extract regular\n",
      "\n",
      "\n",
      "원문 : tried snack pack starbucks well must special cracker mad cuz yes huge box nasty crackers even took work people eat anything cant give away \n",
      "실제 요약 : crackers \n",
      "예측 요약 :  would bitterness smelly ramen ramen primal san\n",
      "\n",
      "\n",
      "원문 : never much soup person unless cold tired cook something fancy heated crumbled tortilla chips top since love mexican food satisfied evening advised progresso discontinued soup thank goodness amazon purchasing \n",
      "실제 요약 : chicken cheese soup please come back \n",
      "예측 요약 :  would bitterness smelly ramen ramen primal san\n",
      "\n",
      "\n",
      "원문 : old mother biscuits true biscuits dogs like would eat whole bag let thing like contain wheat long dog wheat allergy big deal would definitely recommend \n",
      "실제 요약 : my pups them down \n",
      "예측 요약 :  diabetics family england purpose packing without medium\n",
      "\n",
      "\n",
      "원문 : cannot say good kobe steak tastes like package received spoiled greatly dismayed handling expensive delivery try kobe fine restaurant instead home grill \n",
      "실제 요약 : disappointed \n",
      "예측 요약 :  would bitterness smelly ramen ramen primal deceptive\n",
      "\n",
      "\n",
      "원문 : love soup add extra cheese crackers fills love flavors offered especially one tomato basil buy case really like delivered door \n",
      "실제 요약 : love it \n",
      "예측 요약 :  would england bitterness smelly ramen ramen primal\n",
      "\n",
      "\n",
      "원문 : unfortunately flavor peanut butter oil remove tell put water replace end peanut butter end thin slightly peanut flavored stuff buy expect anything else \n",
      "실제 요약 : not for peanut butter \n",
      "예측 요약 :  would bitterness smelly ramen ramen primal san\n",
      "\n",
      "\n",
      "원문 : adore costa rican coffee although absolute favorite brand cheaper others costa rican coffee wonderful still wonderful tropical allow right moisture soil without acidity also generally dark roasted flavor smooth wonderful \n",
      "실제 요약 : worth try \n",
      "예측 요약 :  would lavazza terrific purpose toy tin stix\n",
      "\n",
      "\n",
      "원문 : tea advertised tea tin packaging also excellent condition tea tastes nice like normal english breakfast non string tea bags round instead lovely gift \n",
      "실제 요약 : lovely \n",
      "예측 요약 :  diabetics brilliant then chaser lollipops nature together\n",
      "\n",
      "\n",
      "원문 : good millstone columbian average product average bean quality taste like pods use cheaper lower quality beans \n",
      "실제 요약 : good flavor and average coffee bean quality \n",
      "예측 요약 :  would england bitterness smelly ramen ramen organic\n",
      "\n",
      "\n",
      "원문 : tried many whole grain cereals cannot remember honest many best forgotten kashi easily best kashi many interesting mixture textures black currants magic ingredient cool sweetness bring party makes cereal add little bit silk light vanilla soy milk breakfast bliss \n",
      "실제 요약 : incredibly tasty cereal \n",
      "예측 요약 :  bitterness gourmet kcup jug jug inexpensive condition\n",
      "\n",
      "\n",
      "원문 : son loves maple brown sugar cream wheat offered try quaker maple brown sugar oatmeal jumped chance offer something different huge hit super easy prepare put oatmeal bowl cup milk put microwave one minute ate almost quick made great price box instant oatmeal subscribe save save even cancel change shipment anytime \n",
      "실제 요약 : delicious and easy to prepare \n",
      "예측 요약 :  bitterness prices coffees smooth peggy snacks snacks\n",
      "\n",
      "\n",
      "원문 : haribo best gummi candy love pink grapefruit hard find unless candy store haribo best candy amazon best price \n",
      "실제 요약 : haribo best \n",
      "예측 요약 :  would england bitterness smelly ramen ramen primal\n",
      "\n",
      "\n",
      "원문 : knew could put buffalo wings bag pretzel pieces absolutely wonderful less fattening actual chicken wings less fat yummy yummy yummy \n",
      "실제 요약 : holy hot sauce \n",
      "예측 요약 :  diabetics china filters past taste lemonade lemonade\n",
      "\n",
      "\n",
      "원문 : went gf three months ago slowly researching different mixes find like best happened first mix picked whole foods husband loved much brownies easy mix tasty trying gf brownie mixes found one mixes actually comes mixes like bob red mill cake like definitely recommend mix \n",
      "실제 요약 : great gluten free brownies \n",
      "예측 요약 :  bitterness prices coffees smooth canidae mm addictive\n",
      "\n",
      "\n",
      "원문 : like crispbreads convenient shipped door price less local supermarket sometimes shipper around amazon shipping department puts air shipping carton way product move around box maybe two three edges carton gets edges crackers stay intact shipper box edges crackers crumble six edges would work better \n",
      "실제 요약 : crackers vs cracker crumbs \n",
      "예측 요약 :  bitterness keeps oat kashi superb online kitchen\n",
      "\n",
      "\n",
      "원문 : great tasting low fat product available supermarkets area favorite salt pepper product right always skeptical soy based snacks regarding taste salt pepper variety bbq fit bill \n",
      "실제 요약 : great low fat snack \n",
      "예측 요약 :  diabetics brilliant then cheap us us pg\n",
      "\n",
      "\n",
      "원문 : tastes amazingly great vanilla flavor definitely best chai probably best one comes top list \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract brew extract\n",
      "\n",
      "\n",
      "원문 : tea disappoint new favorite love light flavorful quality continue buy price cannot beat amazon free shipping fast delivery \n",
      "실제 요약 : my new favorite \n",
      "예측 요약 :  would england bitterness smelly ramen ramen primal\n",
      "\n",
      "\n",
      "원문 : sure authentic cosmic catnip lot larger bits stems coarse material cats loved stuff gotten small sample local pet store thought would great idea get larger size catnip cats barely like stuff even hurts fingers pinch release aroma would recommend oz size \n",
      "실제 요약 : not the same as store bought \n",
      "예측 요약 :  bitterness keeps then wellness together totally tender\n",
      "\n",
      "\n",
      "원문 : excellent product bought price doubled price guess skim better anyway living country immediate access grocery stores shelf stable milk life saver keep least one refrigerator times \n",
      "실제 요약 : bought instead of \n",
      "예측 요약 :  would lavazza terrific england purpose toy stix\n",
      "\n",
      "\n",
      "원문 : husband loves drink keep pitcher fridge year round nice able buy quart size mix little light scoops per quarts water \n",
      "실제 요약 : this is great size \n",
      "예측 요약 :  would useful serving useful serving this superior\n",
      "\n",
      "\n",
      "원문 : chihuahua pomeranian get excited bring teenie dental chews meal hope always stock buy frequently seems help reduce tartar teeth exceptionally high protein unlike treats \n",
      "실제 요약 : my dogs go crazy for greenies \n",
      "예측 요약 :  diabetics family england purpose packing sold cold\n",
      "\n",
      "\n",
      "원문 : greatest way make tea since tea pots great products way \n",
      "실제 요약 : great tea and fast way to have it \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : nuts fresh would give stars could known given sit warehouse till get bought definitely stay away would returned opened already \n",
      "실제 요약 : not fresh at all \n",
      "예측 요약 :  would england bitterness smelly ramen ramen organic\n",
      "\n",
      "\n",
      "원문 : tried gluten free breads crumble try make sandwich tried brand moist easy cut kept fridge week stayed good last piece worked great bread machine look forward trying variations pizza crust \n",
      "실제 요약 : this is good \n",
      "예측 요약 :  diabetics dispenser dispenser dispenser seeds thick ew\n",
      "\n",
      "\n",
      "원문 : disgusting cup ever tasted instant coffee tasted better zero chocolate flavor even worse coffee would recommend one even bother trying \n",
      "실제 요약 : worst cup of them all \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : absolutely fantastic rich smooth love much wish could get locally hopefully get stock great taste quality perfect \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : first ounce bag wonderful tea almost back beautiful red color tastes like fresh strawberries cares kiwi party mouth due strawberries one favorite harney sons tea add honey sugar like tea little tart like tart herbal tea like great drink day makes nice iced tea \n",
      "실제 요약 : sons fine teas strawberry kiwi fruit tea \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  versatile usa then versatile emerald own cookie\n",
      "\n",
      "\n",
      "원문 : little salt pepper olive oil couple tbs curry roast min degrees perfect \n",
      "실제 요약 : roasted \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : love taste good flavor crunchy different normal pork rinds yummy would definately buy great buy also \n",
      "실제 요약 : crunchy \n",
      "예측 요약 :  would england bitterness smelly ramen ramen primal\n",
      "\n",
      "\n",
      "원문 : tasty snack seen last two years nyc found online definitely order \n",
      "실제 요약 : pretzels \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : delicious granola bars ever loved healthy good taste light enough filling perfect snack breakfast go \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : spice tricky get good quality one arrived quick flavors spot use marinate fish chicken grill love \n",
      "실제 요약 : great spice at good price \n",
      "예측 요약 :  would england bitterness smelly ramen ramen organic\n",
      "\n",
      "\n",
      "원문 : tried rub steak ounces bottles using dry rub steaks party weeks ago guests impressed flavor also tried fish wow tasted great trying everything love stuff \n",
      "실제 요약 : not just seasoning salt \n",
      "예측 요약 :  would bitterness smelly ramen ramen gritty community\n",
      "\n",
      "\n",
      "원문 : give month old chihuahua puppy really loves eats food seems happy food fact one cats also enjoy trying dog seems thriving \n",
      "실제 요약 : chihuahua puppy approved \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract brew extract\n",
      "\n",
      "\n",
      "원문 : crackers broken crumbs felt like wasted money buying \n",
      "실제 요약 : not happy with this product \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : agree previous review much oatmeal cookie cannot really taste see much oatmeal however really good tasting chocolate chip cookie bad gave stars took long time receive concern heat oatmeal chocolate chips wants good tasting chocolate chip cookie little taste oatmeal cookie \n",
      "실제 요약 : not bad okay \n",
      "예측 요약 :  bitterness pricey healthiest staple solid drinks miss\n",
      "\n",
      "\n",
      "원문 : taste fine bit bland also noted tea leaf pieces larger teabag crumbs certainly whole leaf drink since like mellow white tea warm though appreciate organic tea offering find amazon might try cheaper version organic white tea next usual teabag crumbs since whole leaf anyways \n",
      "실제 요약 : ok but not whole leaf tea \n",
      "예측 요약 :  bitterness prices coffees offer texture texture one\n",
      "\n",
      "\n",
      "원문 : breastfeeding twins wanted little help milk production used days saw significant increase notice side effects take really high doses either able produce enough twins months without needing supplements went little production lot producing lot milk begin fenugreek helped increase enough supplement formula untill months \n",
      "실제 요약 : worked awesome for milk production \n",
      "예측 요약 :  sushi macadamia salty amazingly cheese amazingly going\n",
      "\n",
      "\n",
      "원문 : sauce right combination sweet spicy mild heat perfect using family spicy overpowered way brands grilled pineapple jerk turkey burgers love trouble finding small town stores go internet worth \n",
      "실제 요약 : love this sauce \n",
      "예측 요약 :  bitterness smelly ramen ramen gritty community waste\n",
      "\n",
      "\n",
      "원문 : cappuccino good cups box expensive still add sugar much happier iced cappuccino make hot instead cold right amount sweetener perfect unfortunately apparently available darn \n",
      "실제 요약 : pricey but good \n",
      "예측 요약 :  would england bitterness smelly ramen ramen organic\n",
      "\n",
      "\n",
      "원문 : dog absolutely loves treats turned quick knowing reward must buy \n",
      "실제 요약 : perfect for training \n",
      "예측 요약 :  jalapeno cheesy cheesy cheesy extract regular regular\n",
      "\n",
      "\n",
      "원문 : okay love orange fingers buy something healthier good tasting even going mention orange fingers oh wait taste good satisfying bit cheaper hard believe free shipping cannot go wrong sell fortune grocery store \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  would lavazza terrific purpose toy together since\n",
      "\n",
      "\n",
      "원문 : truly enjoyed product right blend spice tastes well taco snacks recommend person likes little spice life \n",
      "실제 요약 : very tasty and perfect for tacos with \n",
      "예측 요약 :  would england bitterness smelly ramen ramen organic\n",
      "\n",
      "\n",
      "원문 : first purchased root beer drops local candy store since gone business pleased available amazon simple root beer drop additions wrappers pure pleasure taken course moderation john california \n",
      "실제 요약 : pleasure \n",
      "예측 요약 :  would bitterness smelly ramen ramen primal san\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-thinking",
   "metadata": {},
   "source": [
    "## 추출적 요약 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "white-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "shared-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "miniature-olympus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\r\n",
      "\r\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\r\n",
      "\r\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\r\n",
      "\r\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and find a woman dressed in black, facing away from them. It's Trinity. She brings her hands up from the laptop she's working on at their command.\r\n",
      "\r\n",
      "Outside the hotel a car drives up and three agents appear in neatly pressed black suits. They are Agent Smith (Hugo Weaving), Agent Brown (Paul Goddard), and Agent Jones (Robert Taylor). Agent Smith and the presiding police lieutenant argue. Agent Smith admonishes the policeman that they were given specific orders to contact the agents first, for their\n"
     ]
    }
   ],
   "source": [
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-dover",
   "metadata": {},
   "source": [
    "미리 토큰화하지 않아도 알아서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "split-workshop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-furniture",
   "metadata": {},
   "source": [
    "결과 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "chronic-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "['Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.', 'Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.']\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-banner",
   "metadata": {},
   "source": [
    "단어 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "optimum-thailand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-london",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-component",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-ebony",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
